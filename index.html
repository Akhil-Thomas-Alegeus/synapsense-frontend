<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview Room</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      
      body {
        font-family: 'Inter', 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        background-color: #1a1a1a;
        color: white;
        height: 100vh;
        overflow: hidden;
      }

      /* Main Conference Layout */
      .conference-container {
        display: flex;
        flex-direction: column;
        height: 100vh;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
      }

      /* Header */
      .conference-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 12px 24px;
        background: rgba(0, 0, 0, 0.3);
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
      }

      .meeting-info {
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .meeting-title {
        font-size: 16px;
        font-weight: 600;
      }

      .meeting-id {
        font-size: 12px;
        color: rgba(255, 255, 255, 0.6);
        background: rgba(255, 255, 255, 0.1);
        padding: 4px 10px;
        border-radius: 4px;
      }

      .header-controls {
        display: flex;
        align-items: center;
        gap: 16px;
      }

      .recording-indicator {
        display: none;
        align-items: center;
        gap: 8px;
        background: rgba(234, 67, 53, 0.2);
        padding: 6px 12px;
        border-radius: 20px;
        font-size: 13px;
      }

      .recording-indicator.active {
        display: flex;
      }

      .recording-dot {
        width: 8px;
        height: 8px;
        background: #ea4335;
        border-radius: 50%;
        animation: blink 1s infinite;
      }

      @keyframes blink {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.3; }
      }

      .time-display {
        font-size: 14px;
        color: rgba(255, 255, 255, 0.8);
      }

      /* Video Grid */
      .video-grid {
        flex: 1;
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 16px;
        padding: 20px;
        max-height: calc(100vh - 200px);
      }

      .participant-tile {
        position: relative;
        background: #2d2d3a;
        border-radius: 12px;
        overflow: hidden;
        display: flex;
        align-items: center;
        justify-content: center;
        min-height: 300px;
        border: 2px solid transparent;
        transition: border-color 0.3s ease;
      }

      .participant-tile.speaking {
        border-color: #4ade80;
        box-shadow: 0 0 20px rgba(74, 222, 128, 0.3);
      }

      /* AI Listening State - shows when AI is interrupted and listening */
      .participant-tile.listening {
        border-color: #3b82f6;
        box-shadow: 0 0 20px rgba(59, 130, 246, 0.3);
      }

      .participant-tile.listening::after {
        content: 'üëÇ Listening...';
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        background: rgba(59, 130, 246, 0.9);
        color: white;
        padding: 12px 24px;
        border-radius: 24px;
        font-size: 16px;
        font-weight: 600;
        z-index: 10;
        animation: pulse-listen 1.5s ease-in-out infinite;
      }

      @keyframes pulse-listen {
        0%, 100% { transform: translate(-50%, -50%) scale(1); opacity: 1; }
        50% { transform: translate(-50%, -50%) scale(1.05); opacity: 0.8; }
      }

      .participant-tile video {
        width: 100%;
        height: 100%;
        object-fit: cover;
        position: absolute;
        top: 0;
        left: 0;
      }

      .participant-name {
        position: absolute;
        bottom: 12px;
        left: 12px;
        background: rgba(0, 0, 0, 0.7);
        padding: 6px 12px;
        border-radius: 6px;
        font-size: 13px;
        font-weight: 500;
        display: flex;
        align-items: center;
        gap: 8px;
        z-index: 10;
      }

      .participant-name .mic-icon {
        width: 16px;
        height: 16px;
      }

      .participant-status {
        position: absolute;
        top: 12px;
        right: 12px;
        background: rgba(0, 0, 0, 0.6);
        padding: 4px 10px;
        border-radius: 4px;
        font-size: 11px;
        z-index: 10;
      }

      /* AI Avatar Placeholder with Audio Visualizer */
      .ai-avatar-placeholder {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        z-index: 5;
      }

      .avatar-circle {
        width: 180px;
        height: 180px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        display: flex;
        align-items: center;
        justify-content: center;
        position: relative;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.4);
      }

      .avatar-initials {
        font-size: 64px;
        font-weight: 700;
        color: white;
        text-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
      }

      /* Audio Visualizer Rings */
      .audio-rings {
        position: absolute;
        width: 100%;
        height: 100%;
        pointer-events: none;
      }

      .audio-ring {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        border-radius: 50%;
        border: 3px solid rgba(102, 126, 234, 0.5);
        opacity: 0;
      }

      .audio-ring-1 { width: 200px; height: 200px; }
      .audio-ring-2 { width: 240px; height: 240px; }
      .audio-ring-3 { width: 280px; height: 280px; }

      .ai-avatar-placeholder.speaking .audio-ring {
        animation: pulse-ring 1.5s ease-out infinite;
      }

      .ai-avatar-placeholder.speaking .audio-ring-1 { animation-delay: 0s; }
      .ai-avatar-placeholder.speaking .audio-ring-2 { animation-delay: 0.3s; }
      .ai-avatar-placeholder.speaking .audio-ring-3 { animation-delay: 0.6s; }

      @keyframes pulse-ring {
        0% {
          opacity: 0.8;
          transform: translate(-50%, -50%) scale(0.9);
        }
        100% {
          opacity: 0;
          transform: translate(-50%, -50%) scale(1.3);
        }
      }

      /* Audio Bars Visualizer */
      .audio-bars {
        display: flex;
        align-items: flex-end;
        gap: 4px;
        height: 40px;
        margin-top: 20px;
      }

      .audio-bar {
        width: 6px;
        background: linear-gradient(to top, #4ade80, #22c55e);
        border-radius: 3px;
        transition: height 0.1s ease;
        height: 8px;
      }

      .ai-avatar-placeholder.speaking .audio-bar {
        animation: audio-wave 0.5s ease-in-out infinite alternate;
      }

      .ai-avatar-placeholder.speaking .audio-bar:nth-child(1) { animation-delay: 0.0s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(2) { animation-delay: 0.1s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(3) { animation-delay: 0.2s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(4) { animation-delay: 0.15s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(5) { animation-delay: 0.05s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(6) { animation-delay: 0.25s; }
      .ai-avatar-placeholder.speaking .audio-bar:nth-child(7) { animation-delay: 0.1s; }

      @keyframes audio-wave {
        0% { height: 8px; }
        100% { height: 35px; }
      }

      /* User Tile */
      .user-video-placeholder {
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        z-index: 5;
      }

      .user-video-placeholder.hidden {
        display: none;
      }

      #userVideo {
        width: 100%;
        height: 100%;
        object-fit: cover;
        position: absolute;
        top: 0;
        left: 0;
        transform: scaleX(-1); /* Mirror the video for natural feel */
      }

      .user-avatar {
        width: 140px;
        height: 140px;
        border-radius: 50%;
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        display: flex;
        align-items: center;
        justify-content: center;
        box-shadow: 0 10px 40px rgba(240, 147, 251, 0.3);
      }

      .user-avatar-initials {
        font-size: 48px;
        font-weight: 700;
        color: white;
      }

      /* Transcript Panel */
      .transcript-panel {
        background: rgba(0, 0, 0, 0.4);
        border-top: 1px solid rgba(255, 255, 255, 0.1);
        padding: 16px 24px;
        max-height: 120px;
        overflow-y: auto;
      }

      .transcript-messages {
        display: flex;
        flex-direction: column;
        gap: 8px;
      }

      .transcript-message {
        display: flex;
        gap: 12px;
        align-items: flex-start;
      }

      .transcript-label {
        font-size: 12px;
        font-weight: 600;
        min-width: 80px;
        padding: 4px 8px;
        border-radius: 4px;
      }

      .transcript-label.ai {
        background: rgba(102, 126, 234, 0.3);
        color: #a5b4fc;
      }

      .transcript-label.user {
        background: rgba(240, 147, 251, 0.3);
        color: #f5a5c8;
      }

      .transcript-text {
        font-size: 14px;
        color: rgba(255, 255, 255, 0.9);
        line-height: 1.5;
      }

      /* Analysis Modal */
      .analysis-modal {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(0, 0, 0, 0.8);
        z-index: 1000;
        display: flex;
        align-items: center;
        justify-content: center;
      }
      
      .analysis-content {
        background: linear-gradient(135deg, #1e1e2e 0%, #2d2d3a 100%);
        border-radius: 16px;
        width: 90%;
        max-width: 700px;
        max-height: 85vh;
        overflow-y: auto;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
      }
      
      .analysis-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: 20px 24px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
      }
      
      .analysis-header h2 {
        margin: 0;
        font-size: 20px;
      }
      
      .close-modal {
        background: none;
        border: none;
        color: white;
        font-size: 28px;
        cursor: pointer;
        opacity: 0.7;
        transition: opacity 0.2s;
      }
      
      .close-modal:hover {
        opacity: 1;
      }
      
      .analysis-body {
        padding: 24px;
      }
      
      .loading-analysis {
        text-align: center;
        padding: 40px;
      }
      
      .spinner {
        width: 50px;
        height: 50px;
        border: 4px solid rgba(255, 255, 255, 0.2);
        border-top-color: #4ade80;
        border-radius: 50%;
        animation: spin 1s linear infinite;
        margin: 0 auto 20px;
      }
      
      @keyframes spin {
        to { transform: rotate(360deg); }
      }
      
      .score-section {
        text-align: center;
        margin-bottom: 24px;
      }
      
      .overall-score {
        font-size: 64px;
        font-weight: 700;
        background: linear-gradient(135deg, #4ade80 0%, #22d3ee 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 8px;
      }
      
      .score-label {
        color: rgba(255, 255, 255, 0.6);
        font-size: 14px;
      }
      
      .recommendation-badge {
        display: inline-block;
        padding: 8px 20px;
        border-radius: 20px;
        font-weight: 600;
        margin-top: 16px;
      }
      
      .recommendation-badge.hire {
        background: rgba(74, 222, 128, 0.2);
        color: #4ade80;
      }
      
      .recommendation-badge.maybe {
        background: rgba(234, 179, 8, 0.2);
        color: #eab308;
      }
      
      .recommendation-badge.no-hire {
        background: rgba(239, 68, 68, 0.2);
        color: #ef4444;
      }
      
      .analysis-section {
        background: rgba(0, 0, 0, 0.2);
        border-radius: 12px;
        padding: 16px;
        margin-bottom: 16px;
      }
      
      .analysis-section h3 {
        margin: 0 0 12px 0;
        font-size: 16px;
        color: rgba(255, 255, 255, 0.9);
      }
      
      .analysis-section p {
        margin: 0;
        color: rgba(255, 255, 255, 0.7);
        font-size: 14px;
        line-height: 1.6;
      }
      
      .skill-bar {
        display: flex;
        align-items: center;
        gap: 12px;
        margin-bottom: 8px;
      }
      
      .skill-label {
        width: 100px;
        font-size: 13px;
        color: rgba(255, 255, 255, 0.7);
      }
      
      .skill-progress {
        flex: 1;
        height: 8px;
        background: rgba(255, 255, 255, 0.1);
        border-radius: 4px;
        overflow: hidden;
      }
      
      .skill-progress-fill {
        height: 100%;
        background: linear-gradient(90deg, #4ade80, #22d3ee);
        border-radius: 4px;
        transition: width 0.5s ease;
      }
      
      .skill-score {
        width: 40px;
        text-align: right;
        font-size: 13px;
        font-weight: 600;
      }
      
      .feedback-list {
        list-style: none;
        padding: 0;
        margin: 0;
      }
      
      .feedback-list li {
        padding: 8px 0;
        border-bottom: 1px solid rgba(255, 255, 255, 0.05);
        font-size: 14px;
        color: rgba(255, 255, 255, 0.8);
      }
      
      .feedback-list li:last-child {
        border-bottom: none;
      }
      
      .feedback-list li strong {
        color: #4ade80;
      }
      
      .topics-list {
        display: flex;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 8px;
      }
      
      .topic-tag {
        background: rgba(74, 222, 128, 0.2);
        color: #4ade80;
        padding: 4px 12px;
        border-radius: 12px;
        font-size: 12px;
      }

      /* Control Bar */
      .control-bar {
        display: flex;
        justify-content: center;
        align-items: center;
        gap: 12px;
        padding: 16px 24px;
        background: rgba(0, 0, 0, 0.5);
        border-top: 1px solid rgba(255, 255, 255, 0.1);
      }

      .control-btn {
        width: 56px;
        height: 56px;
        border-radius: 50%;
        border: none;
        cursor: pointer;
        display: flex;
        align-items: center;
        justify-content: center;
        transition: all 0.2s ease;
        position: relative;
      }

      .control-btn svg {
        width: 24px;
        height: 24px;
      }

      .control-btn.primary {
        background: #3b82f6;
        color: white;
      }

      .control-btn.primary:hover {
        background: #2563eb;
        transform: scale(1.05);
      }

      .control-btn.secondary {
        background: #374151;
        color: white;
      }

      .control-btn.secondary:hover {
        background: #4b5563;
      }

      .control-btn.danger {
        background: #ef4444;
        color: white;
      }

      .control-btn.danger:hover {
        background: #dc2626;
      }

      .control-btn.success {
        background: #22c55e;
        color: white;
      }

      .control-btn.success:hover {
        background: #16a34a;
      }

      .control-btn.active {
        background: #22c55e;
      }

      .control-btn.muted {
        background: #ef4444;
      }

      .control-btn.interrupt {
        background: #f59e0b;
        animation: pulse-interrupt 1s infinite;
      }

      .control-btn.interrupt:hover {
        background: #d97706;
      }

      @keyframes pulse-interrupt {
        0%, 100% { box-shadow: 0 0 0 0 rgba(245, 158, 11, 0.5); }
        50% { box-shadow: 0 0 0 10px rgba(245, 158, 11, 0); }
      }

      .control-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .control-btn-label {
        position: absolute;
        bottom: -24px;
        left: 50%;
        transform: translateX(-50%);
        font-size: 11px;
        white-space: nowrap;
        color: rgba(255, 255, 255, 0.7);
      }

      .control-divider {
        width: 1px;
        height: 40px;
        background: rgba(255, 255, 255, 0.2);
        margin: 0 8px;
      }

      /* Hidden elements */
      #avatarAudio {
        display: none;
      }

      #avatarVideo {
        display: none;
        /* Optimize for smooth playback */
        will-change: contents;
        transform: translateZ(0);
        backface-visibility: hidden;
        /* Reduce visual stuttering */
        image-rendering: optimizeSpeed;
        -webkit-transform: translate3d(0,0,0);
      }

      #avatarVideo.connected {
        display: block;
      }

      /* TTS Status Toast */
      .tts-toast {
        position: fixed;
        bottom: 100px;
        left: 50%;
        transform: translateX(-50%);
        background: rgba(0, 0, 0, 0.8);
        padding: 12px 24px;
        border-radius: 8px;
        font-size: 14px;
        z-index: 100;
        opacity: 0;
        transition: opacity 0.3s ease;
        pointer-events: none;
      }

      .tts-toast.visible {
        opacity: 1;
      }

      /* Setup Panel (initially shown) */
      .setup-overlay {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: rgba(0, 0, 0, 0.9);
        display: flex;
        align-items: center;
        justify-content: center;
        z-index: 1000;
        opacity: 1;
        transition: opacity 0.3s ease;
      }

      .setup-overlay.hidden {
        opacity: 0;
        pointer-events: none;
      }

      .setup-panel {
        background: #2d2d3a;
        padding: 40px;
        border-radius: 16px;
        text-align: center;
        max-width: 500px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5);
      }

      .setup-title {
        font-size: 24px;
        font-weight: 700;
        margin-bottom: 8px;
      }

      .setup-subtitle {
        color: rgba(255, 255, 255, 0.6);
        margin-bottom: 32px;
      }

      .setup-steps {
        text-align: left;
        margin-bottom: 32px;
      }

      .setup-step {
        display: flex;
        align-items: center;
        gap: 16px;
        padding: 16px;
        background: rgba(255, 255, 255, 0.05);
        border-radius: 8px;
        margin-bottom: 12px;
      }

      .setup-step-number {
        width: 32px;
        height: 32px;
        background: #3b82f6;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: 600;
        flex-shrink: 0;
      }

      .setup-step-content {
        flex: 1;
      }

      .setup-step-title {
        font-weight: 600;
        margin-bottom: 4px;
      }

      .setup-step-desc {
        font-size: 13px;
        color: rgba(255, 255, 255, 0.6);
      }

      .setup-step .step-btn {
        padding: 8px 16px;
        border: none;
        border-radius: 6px;
        font-size: 13px;
        font-weight: 500;
        cursor: pointer;
        transition: all 0.2s ease;
      }

      .setup-step .step-btn.test {
        background: #22c55e;
        color: white;
      }

      .setup-step .step-btn.test:hover {
        background: #16a34a;
      }

      .setup-step .step-btn.audio {
        background: #f59e0b;
        color: white;
      }

      .setup-step .step-btn.audio:hover {
        background: #d97706;
      }

      .setup-step .step-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      .setup-step .step-status {
        font-size: 12px;
        margin-top: 6px;
      }

      .setup-step .step-status.success {
        color: #4ade80;
      }

      .setup-step .step-status.error {
        color: #f87171;
      }

      .invite-section {
        margin-bottom: 20px;
        padding: 16px;
        background: rgba(59, 130, 246, 0.1);
        border-radius: 12px;
        border: 1px solid rgba(59, 130, 246, 0.2);
      }

      .invite-label {
        display: block;
        font-size: 14px;
        font-weight: 500;
        margin-bottom: 8px;
        color: #e2e8f0;
      }

      .invite-input {
        width: 100%;
        padding: 14px 16px;
        background: rgba(0, 0, 0, 0.3);
        border: 1px solid rgba(255, 255, 255, 0.2);
        border-radius: 8px;
        color: white;
        font-size: 18px;
        font-weight: 600;
        text-align: center;
        letter-spacing: 4px;
        text-transform: uppercase;
      }

      .invite-input:focus {
        outline: none;
        border-color: #3b82f6;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.2);
      }

      .invite-input::placeholder {
        letter-spacing: 2px;
        opacity: 0.5;
      }

      .invite-status {
        margin-top: 8px;
        font-size: 13px;
        text-align: center;
      }

      .invite-status.valid {
        color: #4ade80;
      }

      .invite-status.invalid {
        color: #f87171;
      }

      .invite-status.checking {
        color: #94a3b8;
      }

      .welcome-msg {
        margin-top: 12px;
        padding: 12px;
        background: rgba(74, 222, 128, 0.15);
        border: 1px solid rgba(74, 222, 128, 0.3);
        border-radius: 8px;
        text-align: center;
        font-size: 14px;
        color: #4ade80;
        font-weight: 500;
      }

      .welcome-msg.hidden {
        display: none;
      }

      .candidate-welcome {
        margin-top: 12px;
        padding: 12px;
        background: rgba(74, 222, 128, 0.1);
        border-radius: 8px;
        text-align: center;
        font-size: 14px;
        color: #4ade80;
      }

      .join-btn {
        width: 100%;
        padding: 16px;
        background: linear-gradient(135deg, #3b82f6 0%, #8b5cf6 100%);
        color: white;
        border: none;
        border-radius: 8px;
        font-size: 16px;
        font-weight: 600;
        cursor: pointer;
        transition: all 0.2s ease;
      }

      .join-btn:hover:not(:disabled) {
        transform: translateY(-2px);
        box-shadow: 0 10px 30px rgba(59, 130, 246, 0.4);
      }

      .join-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
      }

      /* Hidden legacy elements */
      #transcript, #question {
        display: none;
      }
    </style>
  </head>
  <body>
    <!-- Setup Overlay -->
    <div class="setup-overlay" id="setupOverlay">
      <div class="setup-panel">
        <h2 class="setup-title">üé§ AI Interview Room</h2>
        <p class="setup-subtitle">Complete setup before joining the interview</p>
        
        <div class="setup-steps">
          <div class="setup-step">
            <div class="setup-step-number">1</div>
            <div class="setup-step-content">
              <div class="setup-step-title">Test Audio Output</div>
              <div class="setup-step-desc">Make sure you can hear the AI interviewer</div>
              <div class="step-status" id="tts-status"></div>
            </div>
            <button class="step-btn test" id="testTTS">üîä Test</button>
          </div>
          
          <div class="setup-step">
            <div class="setup-step-number">2</div>
            <div class="setup-step-content">
              <div class="setup-step-title">Enable Audio Playback</div>
              <div class="setup-step-desc">Required for browser audio permissions</div>
            </div>
            <button class="step-btn audio" id="unmute">üîì Enable</button>
          </div>
        </div>
        
        <!-- Invite Code Input -->
        <div class="invite-section">
          <label class="invite-label">Enter your Interview Code:</label>
          <input type="text" id="inviteCode" class="invite-input" placeholder="e.g., ABC123" maxlength="6" style="text-transform: uppercase;">
          <div id="inviteStatus" class="invite-status"></div>
          <div id="welcomeMsg" class="welcome-msg hidden"></div>
        </div>
        
        <button class="join-btn" id="joinMeeting">Join Interview</button>
        <div id="candidateWelcome" class="candidate-welcome" style="display: none;"></div>
      </div>
    </div>

    <!-- Main Conference UI -->
    <div class="conference-container">
      <!-- Header -->
      <div class="conference-header">
        <div class="meeting-info">
          <span class="meeting-title">üéØ AI Technical Interview</span>
          <span class="meeting-id">Room: INT-2024</span>
        </div>
        <div class="header-controls">
          <div class="recording-indicator" id="recordingIndicator">
            <span class="recording-dot"></span>
            <span>Recording</span>
          </div>
          <span class="time-display" id="questionCounter" style="margin-right: 12px; background: rgba(74, 222, 128, 0.2); padding: 4px 10px; border-radius: 4px;">Q: 0/20</span>
          <span class="time-display" id="meetingTime">00:00</span>
        </div>
      </div>

      <!-- Video Grid -->
      <div class="video-grid">
        <!-- AI Interviewer Tile -->
        <div class="participant-tile" id="aiTile">
          <video id="avatarVideo" autoplay playsinline muted width="640" height="480" style="background: #000;"></video>
          <audio id="avatarAudio" autoplay></audio>
          
          <div class="ai-avatar-placeholder" id="aiPlaceholder">
            <div class="audio-rings">
              <div class="audio-ring audio-ring-1"></div>
              <div class="audio-ring audio-ring-2"></div>
              <div class="audio-ring audio-ring-3"></div>
            </div>
            <div class="avatar-circle">
              <span class="avatar-initials">AI</span>
            </div>
            <div class="audio-bars">
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
              <div class="audio-bar"></div>
            </div>
          </div>
          
          <div class="participant-name">
            <svg class="mic-icon" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
              <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
            </svg>
            AI Interviewer
          </div>
          <div class="participant-status" id="avatar-status">Connecting...</div>
        </div>

        <!-- User Tile -->
        <div class="participant-tile" id="userTile">
          <video id="userVideo" autoplay muted playsinline></video>
          <div class="user-video-placeholder" id="userPlaceholder">
            <div class="user-avatar">
              <span class="user-avatar-initials">You</span>
            </div>
          </div>
          
          <div class="participant-name">
            <svg class="mic-icon" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
              <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
            </svg>
            You (Candidate)
          </div>
          <div class="participant-status" id="user-status">Ready</div>
        </div>
      </div>

      <!-- Transcript Panel -->
      <div class="transcript-panel">
        <div class="transcript-messages" id="transcriptMessages">
          <div class="transcript-message">
            <span class="transcript-label ai">AI</span>
            <span class="transcript-text" id="question">Waiting to start...</span>
          </div>
          <div class="transcript-message">
            <span class="transcript-label user">You</span>
            <span class="transcript-text" id="transcript">Click the microphone to begin</span>
          </div>
        </div>
      </div>

      <!-- Control Bar -->
      <div class="control-bar">
        <button class="control-btn secondary" id="startAvatar" title="Start AI Avatar">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M15 8v8H5V8h10m1-2H4c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h12c.55 0 1-.45 1-1v-3.5l4 4v-11l-4 4V7c0-.55-.45-1-1-1z"/>
          </svg>
          <span class="control-btn-label">Avatar</span>
        </button>
        
        <button class="control-btn secondary" id="stopAvatar" disabled title="Stop AI Avatar">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M21 6.5l-4 4V7c0-.55-.45-1-1-1H9.82L21 17.18V6.5zM3.27 2L2 3.27 4.73 6H4c-.55 0-1 .45-1 1v10c0 .55.45 1 1 1h12c.21 0 .39-.08.54-.18L19.73 21 21 19.73 3.27 2z"/>
          </svg>
          <span class="control-btn-label">Stop</span>
        </button>

        <!-- Interrupt AI Speech Button -->
        <button class="control-btn interrupt" id="interruptBtn" style="display: none;" title="Interrupt AI">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M6 6h12v12H6z"/>
          </svg>
          <span class="control-btn-label">Stop AI</span>
        </button>

        <div class="control-divider"></div>

        <button class="control-btn primary" id="start" title="Start Recording">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.91-3c-.49 0-.9.36-.98.85C16.52 14.2 14.47 16 12 16s-4.52-1.8-4.93-4.15c-.08-.49-.49-.85-.98-.85-.61 0-1.09.54-1 1.14.49 3 2.89 5.35 5.91 5.78V20c0 .55.45 1 1 1s1-.45 1-1v-2.08c3.02-.43 5.42-2.78 5.91-5.78.1-.6-.39-1.14-1-1.14z"/>
          </svg>
          <span class="control-btn-label">Mic</span>
        </button>

        <button class="control-btn muted" id="stop" disabled title="Stop Recording">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M19 11h-1.7c0 .74-.16 1.43-.43 2.05l1.23 1.23c.56-.98.9-2.09.9-3.28zm-4.02.17c0-.06.02-.11.02-.17V5c0-1.66-1.34-3-3-3S9 3.34 9 5v.18l5.98 5.99zM4.27 3L3 4.27l6.01 6.01V11c0 1.66 1.33 3 2.99 3 .22 0 .44-.03.65-.08l1.66 1.66c-.71.33-1.5.52-2.31.52-2.76 0-5.3-2.1-5.3-5.1H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c.91-.13 1.77-.45 2.54-.9L19.73 21 21 19.73 4.27 3z"/>
          </svg>
          <span class="control-btn-label">Mute</span>
        </button>

        <div class="control-divider"></div>

        <button class="control-btn danger" id="endCall" title="End Interview">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 9c-1.6 0-3.15.25-4.6.72v3.1c0 .39-.23.74-.56.9-.98.49-1.87 1.12-2.66 1.85-.18.18-.43.28-.7.28-.28 0-.53-.11-.71-.29L.29 13.08c-.18-.17-.29-.42-.29-.7 0-.28.11-.53.29-.71C3.34 8.78 7.46 7 12 7s8.66 1.78 11.71 4.67c.18.18.29.43.29.71 0 .28-.11.53-.29.71l-2.48 2.48c-.18.18-.43.29-.71.29-.27 0-.52-.11-.7-.28-.79-.74-1.69-1.36-2.67-1.85-.33-.16-.56-.5-.56-.9v-3.1C15.15 9.25 13.6 9 12 9z"/>
          </svg>
          <span class="control-btn-label">End</span>
        </button>
      </div>
    </div>

    <!-- TTS Toast -->
    <div class="tts-toast" id="ttsToast"></div>

    <!-- Hidden status span for compatibility -->
    <span id="status" style="display:none;"></span>
    
    <!-- Load the Speech SDK browser bundle -->
    <script src="https://aka.ms/csspeech/jsbrowserpackageraw"></script>
    <script>
      const sdk = SpeechSDK;
      
      // API Base URL for backend
      const API_BASE = 'https://synapsencebackend-dzceeafvbwgca8br.westus2-01.azurewebsites.net';

      // DOM elements
      const startBtn = document.getElementById('start');
      const stopBtn = document.getElementById('stop');
      const transcriptDiv = document.getElementById('transcript');
      const questionDiv = document.getElementById('question');
      const statusSpan = document.getElementById('status');
      const testTTSBtn = document.getElementById('testTTS');
      const ttsStatus = document.getElementById('tts-status');
      
      // Avatar elements
      const startAvatarBtn = document.getElementById('startAvatar');
      const stopAvatarBtn = document.getElementById('stopAvatar');
      const unmuteBtn = document.getElementById('unmute');
      const avatarVideo = document.getElementById('avatarVideo');
      const avatarAudio = document.getElementById('avatarAudio');
      const avatarStatus = document.getElementById('avatar-status');
      
      // User video elements
      const userVideo = document.getElementById('userVideo');
      const userPlaceholder = document.getElementById('userPlaceholder');
      const userStatus = document.getElementById('user-status');
      let userStream = null;
      
      // Video recording elements
      let mediaRecorder = null;
      let recordedChunks = [];
      let isRecording = false;
      
      // New UI elements
      const setupOverlay = document.getElementById('setupOverlay');
      const joinMeetingBtn = document.getElementById('joinMeeting');
      const inviteCodeInput = document.getElementById('inviteCode');
      const inviteStatus = document.getElementById('inviteStatus');
      const welcomeMsg = document.getElementById('welcomeMsg');
      const recordingIndicator = document.getElementById('recordingIndicator');
      
      // Invite code validation state
      let validatedInvite = null;
      const meetingTimeDisplay = document.getElementById('meetingTime');
      const questionCounterDisplay = document.getElementById('questionCounter');
      const aiPlaceholder = document.getElementById('aiPlaceholder');
      const aiTile = document.getElementById('aiTile');
      const userTile = document.getElementById('userTile');
      const ttsToast = document.getElementById('ttsToast');

      let recognizer = null;
      let avatarSynthesizer = null;
      let peerConnection = null;
      let audioContext = null;
      let meetingStartTime = null;
      let meetingTimer = null;
      
      // Avatar display tracking - only use avatar audio when video is actually showing
      let isAvatarVideoDisplaying = false;
      
      // Global speaking state - track when AI is speaking (avatar or TTS)
      let isSpeaking = false;
      
      // Interview session tracking
      let interviewSessionId = null;
      let questionCount = 0;
      const maxQuestions = 20;
      const maxDurationMinutes = 30;

      // ============ Video Recording Functions ============
      
      function startVideoRecording() {
        if (!userStream || isRecording) return;
        
        try {
          recordedChunks = [];
          const options = { mimeType: 'video/webm;codecs=vp9' };
          
          // Fallback if vp9 not supported
          if (!MediaRecorder.isTypeSupported(options.mimeType)) {
            options.mimeType = 'video/webm;codecs=vp8';
          }
          if (!MediaRecorder.isTypeSupported(options.mimeType)) {
            options.mimeType = 'video/webm';
          }
          
          mediaRecorder = new MediaRecorder(userStream, options);
          
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              recordedChunks.push(event.data);
            }
          };
          
          mediaRecorder.onstop = async () => {
            console.log('Recording stopped, uploading...');
            await uploadVideoRecording();
          };
          
          mediaRecorder.start(1000); // Collect data every second
          isRecording = true;
          console.log('Video recording started');
        } catch (error) {
          console.error('Failed to start recording:', error);
        }
      }
      
      function stopVideoRecording() {
        if (mediaRecorder && isRecording) {
          mediaRecorder.stop();
          isRecording = false;
          console.log('Video recording stopping...');
        }
      }
      
      async function uploadVideoRecording() {
        if (recordedChunks.length === 0 || !interviewSessionId) {
          console.log('No recording data or session ID');
          return;
        }
        
        try {
          showToast('Uploading interview recording... Please wait.', 10000);
          
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          console.log(`Video size: ${(blob.size / 1024 / 1024).toFixed(2)} MB`);
          
          // Use XMLHttpRequest for better large file handling and progress
          const xhr = new XMLHttpRequest();
          
          xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
              const percent = Math.round((e.loaded / e.total) * 100);
              console.log(`Upload progress: ${percent}%`);
            }
          };
          
          xhr.onload = () => {
            if (xhr.status === 200) {
              console.log('Video uploaded successfully');
              showToast('Recording saved successfully!', 3000);
            } else {
              console.error('Upload failed:', xhr.status);
              showToast('Failed to save recording (server error)', 3000);
            }
          };
          
          xhr.onerror = () => {
            console.error('Upload network error');
            showToast('Upload failed - network error. Video saved locally.', 5000);
            // Save locally as fallback
            saveVideoLocally(blob);
          };
          
          xhr.ontimeout = () => {
            console.error('Upload timeout');
            showToast('Upload timed out. Video saved locally.', 5000);
            saveVideoLocally(blob);
          };
          
          xhr.timeout = 300000; // 5 minute timeout
          xhr.open('POST', `${API_BASE}/api/storage/upload-video/${interviewSessionId}`);
          xhr.setRequestHeader('Content-Type', 'video/webm');
          xhr.send(blob);
          
        } catch (error) {
          console.error('Upload error:', error);
          showToast('Failed to upload recording', 3000);
        }
        
        recordedChunks = [];
      }
      
      // Save video locally as fallback
      function saveVideoLocally(blob) {
        try {
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = `interview-${interviewSessionId}.webm`;
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
          console.log('Video saved locally');
        } catch (e) {
          console.error('Failed to save locally:', e);
        }
      }

      // ============ User Video Functions ============
      
      async function startUserVideo() {
        try {
          // Check if mediaDevices is available (requires HTTPS or localhost)
          if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            console.error('MediaDevices API not available. Page must be served over HTTPS.');
            userStatus.textContent = 'HTTPS required';
            showToast('Camera requires HTTPS. Use https://localhost or https://127.0.0.1', 5000);
            return;
          }
          
          userStream = await navigator.mediaDevices.getUserMedia({ 
            video: { 
              width: { ideal: 1280 },
              height: { ideal: 720 },
              facingMode: 'user'
            }, 
            audio: false // Audio handled separately by speech recognition
          });
          
          userVideo.srcObject = userStream;
          userPlaceholder.classList.add('hidden');
          userStatus.textContent = 'Camera On';
          console.log('User video started');
        } catch (err) {
          console.error('Failed to start user video:', err);
          
          // Provide specific error messages
          if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
            userStatus.textContent = 'Camera blocked';
            showToast('Please allow camera access in your browser settings', 5000);
          } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
            userStatus.textContent = 'No camera found';
            showToast('No camera detected on this device', 5000);
          } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
            userStatus.textContent = 'Camera in use';
            showToast('Camera is being used by another application', 5000);
          } else if (err.name === 'OverconstrainedError') {
            // Try again with simpler constraints
            try {
              userStream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
              userVideo.srcObject = userStream;
              userPlaceholder.classList.add('hidden');
              userStatus.textContent = 'Camera On';
              return;
            } catch (e) {
              userStatus.textContent = 'Camera error';
              showToast('Could not start camera', 5000);
            }
          } else {
            userStatus.textContent = 'Camera unavailable';
            showToast('Camera access denied or unavailable: ' + err.message, 5000);
          }
        }
      }
      
      function stopUserVideo() {
        if (userStream) {
          userStream.getTracks().forEach(track => track.stop());
          userStream = null;
          userVideo.srcObject = null;
          userPlaceholder.classList.remove('hidden');
          userStatus.textContent = 'Camera Off';
        }
      }

      // ============ Invite Code Validation ============
      
      async function validateInviteCode(code) {
        if (!code || code.length < 6) {
          inviteCodeInput.classList.remove('invite-valid', 'invite-error');
          inviteStatus.textContent = '';
          welcomeMsg.classList.add('hidden');
          validatedInvite = null;
          joinMeetingBtn.disabled = true;
          return;
        }
        
        try {
          const response = await fetch(`${API_BASE}/api/invite/validate/${code.toUpperCase()}`);
          const data = await response.json();
          
          if (data.valid) {
            inviteCodeInput.classList.remove('invite-error');
            inviteCodeInput.classList.add('invite-valid');
            inviteStatus.textContent = '‚úÖ Valid invite code';
            inviteStatus.style.color = '#4ade80';
            welcomeMsg.textContent = `Welcome, ${data.candidateName}! Position: ${data.position}`;
            welcomeMsg.classList.remove('hidden');
            validatedInvite = { code: code.toUpperCase(), ...data };
            joinMeetingBtn.disabled = false;
          } else {
            inviteCodeInput.classList.remove('invite-valid');
            inviteCodeInput.classList.add('invite-error');
            inviteStatus.textContent = `‚ùå ${data.message || 'Invalid invite code'}`;
            inviteStatus.style.color = '#f87171';
            welcomeMsg.classList.add('hidden');
            validatedInvite = null;
            joinMeetingBtn.disabled = true;
          }
        } catch (err) {
          inviteCodeInput.classList.remove('invite-valid');
          inviteCodeInput.classList.add('invite-error');
          inviteStatus.textContent = '‚ùå Error validating code';
          inviteStatus.style.color = '#f87171';
          welcomeMsg.classList.add('hidden');
          validatedInvite = null;
          joinMeetingBtn.disabled = true;
        }
      }
      
      // Debounce invite code validation
      let inviteValidationTimeout = null;
      inviteCodeInput.addEventListener('input', (e) => {
        clearTimeout(inviteValidationTimeout);
        const code = e.target.value.trim();
        
        // Auto-uppercase
        e.target.value = code.toUpperCase();
        
        inviteValidationTimeout = setTimeout(() => {
          validateInviteCode(code);
        }, 500);
      });
      
      // Disable join button initially
      joinMeetingBtn.disabled = true;

      // ============ Utility Functions ============
      
      function showToast(message, duration = 3000) {
        ttsToast.textContent = message;
        ttsToast.classList.add('visible');
        setTimeout(() => ttsToast.classList.remove('visible'), duration);
      }

      function updateMeetingTime() {
        if (!meetingStartTime) return;
        const elapsed = Math.floor((Date.now() - meetingStartTime) / 1000);
        const mins = Math.floor(elapsed / 60).toString().padStart(2, '0');
        const secs = (elapsed % 60).toString().padStart(2, '0');
        meetingTimeDisplay.textContent = `${mins}:${secs}`;
        
        // Check time limit
        const elapsedMinutes = elapsed / 60;
        const remainingMinutes = maxDurationMinutes - elapsedMinutes;
        
        // Warning at 5 minutes remaining
        if (remainingMinutes <= 5 && remainingMinutes > 4.9) {
          showToast('‚è∞ 5 minutes remaining in the interview', 4000);
          meetingTimeDisplay.style.color = '#fbbf24'; // Yellow warning
        }
        
        // Warning at 1 minute remaining
        if (remainingMinutes <= 1 && remainingMinutes > 0.9) {
          showToast('‚è∞ 1 minute remaining!', 3000);
          meetingTimeDisplay.style.color = '#f87171'; // Red warning
        }
        
        // Time's up!
        if (elapsedMinutes >= maxDurationMinutes) {
          endInterviewDueToTimeLimit();
        }
      }
      
      // End interview when time limit is reached
      async function endInterviewDueToTimeLimit() {
        console.log('‚è∞ Interview time limit reached!');
        
        // Stop the timer
        if (meetingTimer) {
          clearInterval(meetingTimer);
          meetingTimer = null;
        }
        
        // Stop any ongoing AI speech
        await interruptAISpeech(false);
        
        // Speak a closing message
        const closingMessage = "We've reached the end of our allotted time for this interview. Thank you so much for your thoughtful responses today. We'll be in touch soon with next steps. Have a great day!";
        questionDiv.innerText = closingMessage;
        
        await speakWithAvatar(closingMessage);
        
        // End the interview after the closing message
        setTimeout(() => {
          endInterview('time_limit');
        }, 8000); // Give time for closing message
      }
      
      // Centralized interview end function
      async function endInterview(reason = 'manual') {
        console.log(`üèÅ Ending interview. Reason: ${reason}`);
        
        // Stop avatar and recognition
        stopAvatar();
        if (recognizer) {
          recognizer.stopContinuousRecognitionAsync();
        }
        
        // Stop timer if still running
        if (meetingTimer) {
          clearInterval(meetingTimer);
          meetingTimer = null;
        }
        
        // Stop video recording and upload
        if (isRecording) {
          stopVideoRecording();
        }
        
        // Calculate final duration
        const duration = meetingStartTime ? Math.floor((Date.now() - meetingStartTime) / 1000) : 0;
        
        // Mark session complete on backend
        if (interviewSessionId) {
          try {
            await fetch(`${API_BASE}/api/interview/end`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ 
                sessionId: interviewSessionId,
                reason: reason,
                duration: duration,
                questionCount: questionCount
              })
            });
            console.log('‚úÖ Interview session ended on backend');
          } catch (err) {
            console.warn('Failed to end session on backend:', err);
          }
        }
        
        // Show completion message based on reason
        let completionMessage = '';
        switch (reason) {
          case 'time_limit':
            completionMessage = '‚è∞ Interview completed (time limit reached)';
            break;
          case 'question_limit':
            completionMessage = '‚úÖ Interview completed (all questions answered)';
            break;
          case 'manual':
          default:
            completionMessage = '‚úÖ Interview ended';
        }
        
        showToast(completionMessage, 5000);
        avatarStatus.textContent = 'Interview Complete';
        
        // Optionally redirect or show summary
        // window.location.href = '/interview-complete.html';
      }

      function setAISpeaking(speaking) {
        const interruptBtn = document.getElementById('interruptBtn');
        if (speaking) {
          aiPlaceholder.classList.add('speaking');
          aiTile.classList.add('speaking');
          aiTile.classList.remove('listening'); // Remove listening state when speaking
          // Show interrupt button when AI is speaking
          if (interruptBtn) interruptBtn.style.display = 'flex';
        } else {
          aiPlaceholder.classList.remove('speaking');
          aiTile.classList.remove('speaking');
          // Hide interrupt button when AI stops
          if (interruptBtn) interruptBtn.style.display = 'none';
        }
      }

      // Interrupt AI speech
      // voiceInterrupt: true = user spoke to interrupt, false = button/key press
      async function interruptAISpeech(voiceInterrupt = false) {
        console.log('üõë Interrupting AI speech...', voiceInterrupt ? '(voice)' : '(manual)');
        
        // 1. IMMEDIATE: Mute audio for instant silence
        if (avatarAudio) {
          avatarAudio.muted = true;
          console.log('üîá Audio muted immediately');
        }
        
        // 2. Stop Avatar speech (if using avatar)
        if (avatarSynthesizer) {
          try {
            // Try stopSpeakingAsync first
            if (typeof avatarSynthesizer.stopSpeakingAsync === 'function') {
              avatarSynthesizer.stopSpeakingAsync()
                .then(() => console.log('‚úÖ Avatar stopSpeakingAsync completed'))
                .catch(e => console.log('stopSpeakingAsync error:', e.message));
            }
            
            // Also try to cancel any pending synthesis
            if (typeof avatarSynthesizer.close === 'function') {
              // Don't close - we want to reuse it, but cancellation might help
              console.log('üé≠ Avatar synthesizer available for next speech');
            }
          } catch (e) {
            console.log('Avatar stop error:', e.message);
          }
        }
        
        // 3. Stop TTS speech (if using TTS fallback)
        if (speechSynthesizer) {
          try {
            speechSynthesizer.close();
            speechSynthesizer = null;
            console.log('‚úÖ TTS speech stopped');
          } catch (e) {
            console.log('TTS stop error:', e.message);
          }
        }
        
        // 4. Reset speaking state
        setAISpeaking(false);
        isSpeaking = false;
        
        // 5. Unmute audio after short delay (so next speech works)
        setTimeout(() => {
          if (avatarAudio && audioUnlocked) {
            avatarAudio.muted = false;
            console.log('üîä Audio unmuted');
          }
        }, 500);
        
        // 6. Set appropriate status based on interrupt type
        if (voiceInterrupt) {
          wasInterrupted = true; // Mark that we were interrupted by voice
          pendingUserSpeech = null; // Clear any pending speech - we're starting fresh
          avatarStatus.textContent = 'Listening...';
          console.log('‚úÖ AI interrupted by voice - now listening');
          
          // Add visual "listening" state to AI tile
          aiTile.classList.add('listening');
          
          // Remove listening state after user stops speaking (or timeout)
          setTimeout(() => {
            aiTile.classList.remove('listening');
          }, 4000);
        } else {
          // Manual interrupt - clear state
          wasInterrupted = false;
          pendingUserSpeech = null;
          avatarStatus.textContent = 'Ready';
          showToast('‚úÖ AI stopped', 1500);
          console.log('‚úÖ AI speech interrupted manually');
          aiTile.classList.remove('listening');
        }
      }

      // Wire up interrupt button (manual = false)
      document.getElementById('interruptBtn')?.addEventListener('click', () => interruptAISpeech(false));

      // Keyboard shortcut to interrupt AI (Escape key) - manual = false
      document.addEventListener('keydown', (e) => {
        // Only interrupt if AI is speaking and not in setup overlay
        if (e.key === 'Escape' && aiTile.classList.contains('speaking')) {
          e.preventDefault();
          interruptAISpeech(false);
        }
      });

      function setUserSpeaking(speaking) {
        if (speaking) {
          userTile.classList.add('speaking');
        } else {
          userTile.classList.remove('speaking');
        }
      }

      // ============ Join Meeting ============
      
      joinMeetingBtn.onclick = async () => {
        // Ensure invite code is validated
        if (!validatedInvite) {
          inviteStatus.textContent = '‚ùå Please enter a valid invite code';
          inviteStatus.style.color = '#f87171';
          return;
        }
        
        setupOverlay.classList.add('hidden');
        meetingStartTime = Date.now();
        meetingTimer = setInterval(updateMeetingTime, 1000);
        
        // Start interview session on backend with invite code
        try {
          const sessionResp = await fetch(`${API_BASE}/api/interview/start`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ inviteCode: validatedInvite.code })
          });
          if (sessionResp.ok) {
            const sessionData = await sessionResp.json();
            interviewSessionId = sessionData.sessionId;
            questionCount = 0;
            console.log('üìã Interview session started:', interviewSessionId, 'for candidate:', validatedInvite.candidateName);
          }
        } catch (err) {
          console.warn('Could not start interview session:', err);
        }
        
        // Start user's camera
        await startUserVideo();
        
        // Start video recording after camera is ready
        setTimeout(() => {
          startVideoRecording();
        }, 1000);
        
        // Unlock audio (required by browser policy) - user clicked so we can do this
        try {
          if (!audioContext) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
          }
          if (audioContext.state === 'suspended') {
            await audioContext.resume();
          }
          // Play a silent buffer to fully unlock audio
          const buffer = audioContext.createBuffer(1, 1, 22050);
          const source = audioContext.createBufferSource();
          source.buffer = buffer;
          source.connect(audioContext.destination);
          source.start();
          audioUnlocked = true;
          console.log('üîä Audio unlocked automatically');
        } catch (e) {
          console.warn('Could not auto-unlock audio:', e);
        }
        
        // Auto-start the AI Avatar
        showToast('üé¨ Starting AI Avatar...');
        try {
          await startAvatar();
          console.log('üé≠ Avatar started automatically');
        } catch (err) {
          console.warn('Could not auto-start avatar, falling back to TTS:', err);
          // Fallback to TTS welcome message if avatar fails
          setTimeout(() => {
            speakWithTTS("Welcome to your AI interview session. When you're ready, click the microphone button to start speaking. I'll listen to your responses and ask follow-up questions.");
          }, 500);
        }
      };

      // ============ Test TTS Function ============
      testTTSBtn.onclick = async () => {
        testTTSBtn.disabled = true;
        ttsStatus.textContent = 'Testing...';
        ttsStatus.className = 'step-status';
        
        try {
          const { token, region } = await fetchSpeechToken();
          
          const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
          speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural";
          
          const player = new sdk.SpeakerAudioDestination();
          
          player.onAudioStart = () => {
            setAISpeaking(true);
          };
          
          player.onAudioEnd = () => {
            setAISpeaking(false);
            ttsStatus.textContent = '‚úÖ Audio works!';
            ttsStatus.className = 'step-status success';
            testTTSBtn.disabled = false;
            testTTSBtn.textContent = '‚úÖ Done';
          };
          
          const audioConfig = sdk.AudioConfig.fromSpeakerOutput(player);
          const synthesizer = new sdk.SpeechSynthesizer(speechConfig, audioConfig);
          
          setAISpeaking(true);
          
          synthesizer.speakTextAsync(
            "Audio test successful. You're ready to join.",
            result => {
              // Don't stop animation here - wait for onAudioEnd
              if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {
                // Set a safety timeout
                setTimeout(() => {
                  setAISpeaking(false);
                  ttsStatus.textContent = '‚úÖ Audio works!';
                  ttsStatus.className = 'step-status success';
                  testTTSBtn.disabled = false;
                  testTTSBtn.textContent = '‚úÖ Done';
                }, 5000);
              } else {
                setAISpeaking(false);
                ttsStatus.textContent = '‚ùå Failed';
                ttsStatus.className = 'step-status error';
                testTTSBtn.disabled = false;
              }
              synthesizer.close();
            },
            error => {
              setAISpeaking(false);
              ttsStatus.textContent = '‚ùå Error';
              ttsStatus.className = 'step-status error';
              synthesizer.close();
              testTTSBtn.disabled = false;
            }
          );
        } catch (error) {
          setAISpeaking(false);
          ttsStatus.textContent = '‚ùå Error';
          ttsStatus.className = 'step-status error';
          testTTSBtn.disabled = false;
        }
      };

      // ============ Avatar Functions ============

      async function fetchSpeechToken() {
        const res = await fetch('https://synapsencebackend-dzceeafvbwgca8br.westus2-01.azurewebsites.net/api/speech/token');
        return res.json();
      }

      async function fetchIceCredentials() {
        const res = await fetch('https://synapsencebackend-dzceeafvbwgca8br.westus2-01.azurewebsites.net/api/speech/ice');
        return res.json();
      }

      // Unmute button handler - required for browser autoplay policy
      let audioUnlocked = false;
      unmuteBtn.onclick = async () => {
        try {
          if (!audioContext) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
          }
          if (audioContext.state === 'suspended') {
            await audioContext.resume();
          }
          
          const buffer = audioContext.createBuffer(1, 1, 22050);
          const source = audioContext.createBufferSource();
          source.buffer = buffer;
          source.connect(audioContext.destination);
          source.start();
          
          avatarVideo.muted = false;
          avatarVideo.volume = 1.0;
          avatarAudio.muted = false;
          avatarAudio.volume = 1.0;
          
          audioUnlocked = true;
          unmuteBtn.textContent = '‚úÖ Done';
          unmuteBtn.disabled = true;
          unmuteBtn.style.background = '#22c55e';
          
        } catch (e) {
          console.error('Failed to unlock audio:', e);
        }
      };

      async function startAvatar() {
        try {
          avatarStatus.textContent = 'Connecting...';
          showToast('üé¨ Starting AI Avatar...');
          
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
          
          // Get speech config - we need region and key info
          const { token, region } = await fetchSpeechToken();
          console.log('üé≠ Avatar: Got token for region:', region);
          
          // Get avatar relay token (ICE credentials) - this is the key for WebRTC!
          const iceInfo = await fetchIceCredentials();
          console.log('üé≠ Avatar: Got ICE relay info:', iceInfo);
          
          // Create speech config using the authorization token
          const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
          speechConfig.speechSynthesisLanguage = "en-US";
          // Use a voice that works well with the lisa avatar for lip-sync
          speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural";

          // Avatar config - using lisa with casual-sitting style
          // Available avatars: lisa, jenny, aria, guy, harry, etc.
          // Available styles: casual-sitting, graceful-sitting, technical-sitting, etc.
          const avatarConfig = new sdk.AvatarConfig("lisa", "casual-sitting");
          avatarConfig.backgroundColor = '#FFFFFFFF'; // White background
          avatarConfig.customized = false;
          
          console.log('üé≠ Avatar: Config - character: lisa, style: casual-sitting, voice: en-US-JennyNeural');
          
          // Setup WebRTC peer connection with ICE servers from Azure relay
          const iceServerUrl = iceInfo.Urls[0];
          const iceServerUsername = iceInfo.Username;
          const iceServerCredential = iceInfo.Password;
          
          console.log('üé≠ Avatar: Setting up WebRTC with ICE server:', iceServerUrl);
          
          peerConnection = new RTCPeerConnection({
            iceServers: [{
              urls: [iceServerUrl],
              username: iceServerUsername,
              credential: iceServerCredential
            }],
            // Optimize for quality over latency
            bundlePolicy: 'max-bundle',
            rtcpMuxPolicy: 'require'
          });

          peerConnection.ontrack = function(event) {
            console.log('üé≠ Avatar: ontrack event received:', event.track.kind);
            
            // Try to increase jitter buffer for smoother playback
            const receiver = event.receiver;
            if (receiver && receiver.jitterBufferTarget !== undefined) {
              // Request larger jitter buffer (in milliseconds)
              // Increased to 500ms to handle 38ms jitter + packet loss
              receiver.jitterBufferTarget = 500; 
              console.log('üé≠ Set jitter buffer target to 500ms');
            }
            
            // Also try to set playout delay hint if available
            if (receiver && receiver.playoutDelayHint !== undefined) {
              receiver.playoutDelayHint = 0.3; // 300ms playout delay
              console.log('üé≠ Set playout delay hint to 300ms');
            }
            
            if (event.track.kind === 'video') {
              console.log('üé≠ Avatar: Setting video srcObject');
              
              // Apply playback delay for buffering
              const stream = event.streams[0];
              avatarVideo.srcObject = stream;
              avatarVideo.muted = true;
              avatarVideo.classList.add('connected');
              aiPlaceholder.style.display = 'none';
              
              // Optimize video element for smooth playback
              avatarVideo.autoplay = true;
              avatarVideo.playsInline = true;
              avatarVideo.disablePictureInPicture = true;
              avatarVideo.preload = 'auto';
              
              // Request high performance rendering
              if (avatarVideo.style) {
                avatarVideo.style.objectFit = 'cover';
              }
              
              // Set flag immediately when we get video track - don't wait for play promise
              isAvatarVideoDisplaying = true;
              console.log('‚úÖüé≠ Avatar VIDEO TRACK RECEIVED - isAvatarVideoDisplaying = true');
              
              // Add delay before playing to allow buffer to fill
              // Increased to 300ms to handle packet loss
              setTimeout(() => {
                avatarVideo.play().then(() => {
                  console.log('‚úÖüé≠ Avatar VIDEO IS NOW PLAYING (with 300ms buffer)');
                  
                  // Monitor buffer health
                  if (avatarVideo.buffered && avatarVideo.buffered.length > 0) {
                    console.log('üé≠ Video buffer:', avatarVideo.buffered.end(0) - avatarVideo.currentTime, 'seconds');
                  }
                }).catch(e => {
                  console.log('Video autoplay issue:', e);
                  // Don't set to false here - the track is still valid
                });
              }, 300); // 300ms initial buffer delay
            }
            if (event.track.kind === 'audio') {
              console.log('üé≠ Avatar: Setting audio srcObject');
              avatarAudio.srcObject = event.streams[0];
              avatarAudio.autoplay = true;
              avatarAudio.preload = 'auto';
              
              // Sync audio with video delay
              setTimeout(() => {
                if (audioUnlocked) {
                  avatarAudio.muted = false;
                  avatarAudio.volume = 1.0;
                  avatarAudio.play().catch(e => console.log('Audio play failed:', e));
                }
              }, 300); // Match video buffer delay (300ms)
            }
          };

          peerConnection.oniceconnectionstatechange = function() {
            console.log('üé≠ Avatar: ICE connection state:', peerConnection.iceConnectionState);
            if (peerConnection.iceConnectionState === 'connected') {
              avatarStatus.textContent = 'Connected';
              showToast('‚úÖ Avatar connected!');
            } else if (peerConnection.iceConnectionState === 'disconnected' || 
                       peerConnection.iceConnectionState === 'failed') {
              avatarStatus.textContent = 'Disconnected';
              isAvatarVideoDisplaying = false; // Mark avatar as not displaying
              console.error('üé≠ Avatar: ICE connection failed or disconnected');
              console.log('‚ö†Ô∏è isAvatarVideoDisplaying set to FALSE');
              showToast('‚ö†Ô∏è Avatar connection failed - using audio-only TTS.');
            }
          };
          
          peerConnection.onicegatheringstatechange = function() {
            console.log('üé≠ Avatar: ICE gathering state:', peerConnection.iceGatheringState);
          };
          
          peerConnection.onconnectionstatechange = function() {
            console.log('üé≠ Avatar: Connection state:', peerConnection.connectionState);
            
            // Start monitoring stats when connected
            if (peerConnection.connectionState === 'connected') {
              startQualityMonitoring();
            }
          };

          // Quality monitoring function
          function startQualityMonitoring() {
            setInterval(async () => {
              if (!peerConnection || peerConnection.connectionState !== 'connected') return;
              
              try {
                const stats = await peerConnection.getStats();
                stats.forEach(report => {
                  if (report.type === 'inbound-rtp' && report.kind === 'video') {
                    const packetsLost = report.packetsLost || 0;
                    const packetsReceived = report.packetsReceived || 0;
                    const jitter = report.jitter || 0;
                    const framesDropped = report.framesDropped || 0;
                    const framesReceived = report.framesReceived || 0;
                    
                    // Log warning if quality is degrading
                    if (packetsLost > 10 || jitter > 0.05) {
                      console.warn('üé≠ Video quality warning:', {
                        packetsLost,
                        jitter: (jitter * 1000).toFixed(1) + 'ms',
                        framesDropped,
                        framesReceived
                      });
                    }
                  }
                });
              } catch (e) {
                // Ignore stats errors
              }
            }, 5000); // Check every 5 seconds
          }

          // Add transceivers with preferences for quality
          const videoTransceiver = peerConnection.addTransceiver('video', { direction: 'sendrecv' });
          const audioTransceiver = peerConnection.addTransceiver('audio', { direction: 'sendrecv' });
          
          // Try to set codec preferences for better quality
          if (videoTransceiver.setCodecPreferences) {
            const capabilities = RTCRtpReceiver.getCapabilities('video');
            if (capabilities) {
              // Prefer VP9 or H.264 for better quality
              const preferredCodecs = capabilities.codecs.filter(c => 
                c.mimeType.includes('VP9') || c.mimeType.includes('H264')
              );
              if (preferredCodecs.length > 0) {
                try {
                  videoTransceiver.setCodecPreferences(preferredCodecs);
                  console.log('üé≠ Set preferred video codecs:', preferredCodecs.map(c => c.mimeType));
                } catch (e) {
                  console.log('Could not set codec preferences:', e);
                }
              }
            }
          }

          console.log('üé≠ Avatar: Creating AvatarSynthesizer...');
          avatarSynthesizer = new sdk.AvatarSynthesizer(speechConfig, avatarConfig);
          
          // Add event listeners for avatar speech synthesis
          avatarSynthesizer.avatarEventReceived = (s, e) => {
            console.log('üé≠ Avatar Event:', e.description);
            // Avatar events include: AvatarStarted, TalkingStarted, TalkingEnded, etc.
            if (e.description.includes('TalkingStarted')) {
              console.log('üé≠ Avatar: Started talking (lip-sync active)');
              setAISpeaking(true);
              avatarStatus.textContent = 'Speaking...';
            } else if (e.description.includes('TalkingStopped') || e.description.includes('TalkingEnded')) {
              console.log('üé≠ Avatar: Stopped talking');
              setAISpeaking(false);
              avatarStatus.textContent = 'Ready';
            }
          };
          
          avatarSynthesizer.synthesisStarted = (s, e) => {
            console.log('üé≠ Avatar: Synthesis started');
            setAISpeaking(true);
          };
          
          avatarSynthesizer.synthesisCompleted = (s, e) => {
            console.log('üé≠ Avatar: Synthesis completed');
            // Don't immediately stop - wait for TalkingStopped event
          };
          
          avatarSynthesizer.wordBoundary = (s, e) => {
            // This fires for each word - indicates lip-sync is working
            console.log('üé≠ Avatar: Word boundary -', e.text);
          };
          
          console.log('üé≠ Avatar: Starting avatar async...');
          const result = await avatarSynthesizer.startAvatarAsync(peerConnection);
          console.log('üé≠ Avatar: startAvatarAsync result:', result);
          
          // Check if result indicates success
          if (result.reason === sdk.ResultReason.SynthesizingAudioStarted) {
            console.log('üé≠ Avatar: Successfully started');
            avatarStatus.textContent = 'Ready';
          } else if (result.reason === sdk.ResultReason.Canceled) {
            const cancellation = sdk.CancellationDetails.fromResult(result);
            console.error('üé≠ Avatar: Canceled -', cancellation.reason, cancellation.errorDetails);
            avatarStatus.textContent = 'Failed: ' + cancellation.errorDetails;
            showToast('‚ùå Avatar canceled: ' + cancellation.errorDetails);
            return;
          } else {
            console.log('üé≠ Avatar: Result reason:', result.reason);
            avatarStatus.textContent = 'Ready';
          }
          
          startAvatarBtn.disabled = true;
          stopAvatarBtn.disabled = false;

          // Wait a moment for video track to be ready, then speak
          await new Promise(resolve => setTimeout(resolve, 1000));
          console.log('üé≠ Avatar ready, isAvatarVideoDisplaying:', isAvatarVideoDisplaying);
          await speakWithAvatar("Hello! I'm your AI interviewer. Click the microphone button to begin.");

        } catch (error) {
          console.error('üé≠ Avatar: Failed to start avatar:', error);
          console.error('üé≠ Avatar: Error details:', error.message, error.stack);
          avatarStatus.textContent = 'Error';
          showToast('‚ùå Avatar failed: ' + error.message);
        }
      }

      async function stopAvatar() {
        try {
          if (avatarSynthesizer) {
            avatarSynthesizer.close();
            avatarSynthesizer = null;
          }
          if (peerConnection) {
            peerConnection.close();
            peerConnection = null;
          }
          avatarVideo.srcObject = null;
          avatarVideo.classList.remove('connected');
          avatarAudio.srcObject = null;
          aiPlaceholder.style.display = 'flex';
          avatarStatus.textContent = 'Disconnected';
          startAvatarBtn.disabled = false;
          stopAvatarBtn.disabled = true;
          
          isAvatarVideoDisplaying = false; // Reset avatar display flag
          audioUnlocked = false;
          console.log('üé≠ Avatar stopped - isAvatarVideoDisplaying set to FALSE');
        } catch (error) {
          console.error('Failed to stop avatar:', error);
        }
      }

      async function speakWithAvatar(text) {
        console.log('üé≠‚û°Ô∏è speakWithAvatar called with:', text.substring(0, 60) + '...');
        console.log('üìä isAvatarVideoDisplaying:', isAvatarVideoDisplaying);
        
        // SET SPEAKING STATE IMMEDIATELY - before any checks or early returns
        // This prevents user speech from being sent to LLM while AI processes
        isSpeaking = true;
        setAISpeaking(true); // Also set visual state
        
        // CRITICAL CHECK: Only use avatar if the video is actually displaying
        if (!isAvatarVideoDisplaying) {
          console.log('‚ö†Ô∏èüîÑ SWITCH TO TTS: Avatar video is NOT displaying');
          console.log('üì¢ Reason: isAvatarVideoDisplaying = false');
          showToast('Using audio mode (avatar not visible)', 2000);
          await speakWithTTS(text);
          return;
        }
        
        // If avatar synthesizer is not available, use TTS fallback
        if (!avatarSynthesizer) {
          console.log('‚ö†Ô∏èüîÑ SWITCH TO TTS: No avatar synthesizer available');
          console.log('üì¢ Reason: avatarSynthesizer is null/undefined');
          isAvatarVideoDisplaying = false; // Reset flag
          showToast('Using audio-only mode (no avatar)', 2000);
          await speakWithTTS(text);
          return;
        }

        // Check if peer connection is still connected
        if (!peerConnection || peerConnection.connectionState === 'closed' || 
            peerConnection.connectionState === 'failed') {
          console.log('‚ö†Ô∏èüîÑ SWITCH TO TTS: Peer connection not ready');
          console.log('üì¢ Reason: peerConnection state =', peerConnection?.connectionState || 'null');
          isAvatarVideoDisplaying = false; // Reset flag
          showToast('Avatar disconnected, using audio', 2000);
          await speakWithTTS(text);
          return;
        }

        console.log('‚úÖüé≠ Avatar VIDEO IS DISPLAYING - using avatar for speech (no TTS)');
        console.log('üìä PeerConnection state:', peerConnection.connectionState);
        console.log('üìä ICE connection state:', peerConnection.iceConnectionState);

        // Ensure audio is playing
        if (avatarAudio.srcObject && audioUnlocked) {
          avatarAudio.muted = false;
          avatarAudio.volume = 1.0;
          avatarAudio.play().catch(e => console.log('Audio play issue:', e));
        }

        try {
          avatarStatus.textContent = 'Speaking...';
          setAISpeaking(true);
          console.log('üé≠ Avatar: Calling speakTextAsync...');
          
          // Try plain text first - sometimes works better for lip-sync
          const result = await avatarSynthesizer.speakTextAsync(text);
          
          console.log('üé≠ Avatar: Speak result reason:', result.reason);
          
          if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {
            console.log('‚úÖüé≠ Avatar speech completed successfully - NO TTS FALLBACK');
            // Let the avatarEventReceived handler manage the speaking state
            // Add a small delay then check if still speaking
            setTimeout(() => {
              isSpeaking = false;
              setAISpeaking(false);
              avatarStatus.textContent = 'Ready';
            }, 500);
            return;
          } else if (result.reason === sdk.ResultReason.Canceled) {
            const cancellation = sdk.CancellationDetails.fromResult(result);
            console.error('‚ùåüé≠ Avatar: Speech canceled -', cancellation.reason, cancellation.errorDetails);
            console.log('‚ö†Ô∏èüîÑ SWITCH TO TTS: Avatar speech was canceled');
            console.log('üì¢ Cancellation reason:', cancellation.reason);
            console.log('üì¢ Error details:', cancellation.errorDetails);
            isAvatarVideoDisplaying = false; // Mark avatar as not working
            isSpeaking = false;
            setAISpeaking(false);
            avatarStatus.textContent = 'Error';
            showToast('Avatar error, using audio', 2000);
            await speakWithTTS(text);
          } else {
            // Other result - might still be working
            console.log('üé≠ Avatar: Unexpected result reason:', result.reason, '- assuming success');
            setTimeout(() => {
              isSpeaking = false;
              setAISpeaking(false);
              avatarStatus.textContent = 'Ready';
            }, 500);
          }
        } catch (error) {
          console.error('‚ùåüé≠ Avatar: speakTextAsync threw exception:', error.message);
          console.log('‚ö†Ô∏èüîÑ SWITCH TO TTS: Avatar threw an error');
          console.log('üì¢ Error:', error);
          isAvatarVideoDisplaying = false; // Mark avatar as not working
          isSpeaking = false;
          setAISpeaking(false);
          avatarStatus.textContent = 'Error';
          showToast('Avatar error, using audio', 2000);
          await speakWithTTS(text);
        }
      }

      // Regular TTS fallback
      let speechSynthesizer = null;
      
      async function speakWithTTS(text) {
        console.log('üîäüì¢ speakWithTTS called - AUDIO ONLY MODE');
        console.log('üîä Text:', text.substring(0, 60) + '...');
        showToast('üîä AI is speaking...');
        isSpeaking = true;
        setAISpeaking(true);
        avatarStatus.textContent = 'Speaking...';
        
        return new Promise(async (resolve, reject) => {
          try {
            const { token, region } = await fetchSpeechToken();
            
            const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
            speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural";
            
            const player = new sdk.SpeakerAudioDestination();
            
            player.onAudioStart = () => {
              console.log('üîä Audio playback started');
              isSpeaking = true;
              setAISpeaking(true);
              avatarStatus.textContent = 'Speaking...';
            };
            
            player.onAudioEnd = () => {
              console.log('üîä Audio playback ended');
              isSpeaking = false;
              setAISpeaking(false);
              avatarStatus.textContent = 'Ready';
              showToast('‚úÖ AI finished speaking');
              resolve();
            };
            
            const audioConfig = sdk.AudioConfig.fromSpeakerOutput(player);
            speechSynthesizer = new sdk.SpeechSynthesizer(speechConfig, audioConfig);
            
            speechSynthesizer.speakTextAsync(
              text,
              result => {
                console.log('üîä Synthesis result:', result.reason);
                if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {
                  // Don't turn off speaking here - wait for onAudioEnd
                  // But set a safety timeout in case onAudioEnd doesn't fire
                  setTimeout(() => {
                    if (isSpeaking) {
                      console.log('üîä Safety timeout - stopping animation');
                      isSpeaking = false;
                      setAISpeaking(false);
                      avatarStatus.textContent = 'Ready';
                    }
                  }, 30000); // 30 second safety timeout
                } else {
                  isSpeaking = false;
                  setAISpeaking(false);
                  avatarStatus.textContent = 'Ready';
                  reject(new Error(result.errorDetails || 'TTS failed'));
                }
                speechSynthesizer.close();
              },
              error => {
                console.error('üîä TTS error:', error);
                isSpeaking = false;
                setAISpeaking(false);
                avatarStatus.textContent = 'Ready';
                speechSynthesizer.close();
                reject(error);
              }
            );
          } catch (error) {
            console.error('üîä Setup error:', error);
            isSpeaking = false;
            setAISpeaking(false);
            avatarStatus.textContent = 'Ready';
            reject(error);
          }
        });
      }

      // Avatar button handlers
      startAvatarBtn.onclick = startAvatar;
      stopAvatarBtn.onclick = stopAvatar;
      
      // End call button
      document.getElementById('endCall').onclick = async () => {
        if (confirm('Are you sure you want to end the interview?')) {
          await endInterview('manual');
          setupOverlay.classList.remove('hidden');
          meetingStartTime = null;
        }
      };

      // ============ Speech Recognition Functions ============

      // Common interruption phrases that are NOT answers
      const interruptionPhrases = [
        'wait', 'hold on', 'stop', 'pause', 'one moment', 'one second', 'one minute',
        'hang on', 'hold up', 'just a moment', 'just a second', 'just a minute',
        'let me think', 'give me a moment', 'give me a second', 'give me a minute',
        'slow down', 'too fast', 'can you repeat', 'repeat that', 'say that again',
        'what was that', 'sorry', 'excuse me', 'um', 'uh', 'hmm', 'okay wait',
        'wait wait', 'hold on hold on', 'stop stop', 'no no', 'actually wait',
        'wait a minute', 'wait a second', 'hold that thought', 'before you continue',
        'can i ask', 'quick question', 'i have a question', 'can you clarify'
      ];

      // Check if speech is just an interruption phrase (not an actual answer)
      function isInterruptionPhrase(text) {
        const normalizedText = text.toLowerCase().trim();
        
        // Check exact matches or starts with common interruptions
        for (const phrase of interruptionPhrases) {
          if (normalizedText === phrase || 
              normalizedText.startsWith(phrase + ' ') ||
              normalizedText.startsWith(phrase + '.') ||
              normalizedText.startsWith(phrase + ',') ||
              normalizedText.startsWith(phrase + '?')) {
            return true;
          }
        }
        
        // If it's very short (< 4 words) and contains interrupt keywords, it's likely an interruption
        const wordCount = normalizedText.split(/\s+/).length;
        if (wordCount <= 4) {
          const interruptKeywords = ['wait', 'stop', 'hold', 'pause', 'moment', 'second', 'minute', 'think', 'repeat'];
          for (const keyword of interruptKeywords) {
            if (normalizedText.includes(keyword)) {
              return true;
            }
          }
        }
        
        return false;
      }

      // Store speech that might be an answer (captured during/after interruption)
      let pendingUserSpeech = null;
      let wasInterrupted = false;

      startBtn.onclick = async () => {
        statusSpan.textContent = 'Connecting...';
        statusSpan.className = '';
        showToast('üé§ Connecting microphone...');
        
        const { region, token } = await fetchSpeechToken();
        const speechConfig = sdk.SpeechConfig.fromAuthorizationToken(token, region);
        speechConfig.speechRecognitionLanguage = "en-US";

        const audioConfig = sdk.AudioConfig.fromDefaultMicrophoneInput();
        recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);

        // Track if we've already interrupted to avoid multiple interrupts
        let hasInterruptedCurrentSpeech = false;

        recognizer.recognizing = (s, e) => {
          const partialText = e.result.text.trim();
          transcriptDiv.innerText = partialText;
          setUserSpeaking(true);
          
          // Check if AI is currently speaking and user starts talking
          if (aiTile.classList.contains('speaking') && partialText.length > 2 && !hasInterruptedCurrentSpeech) {
            console.log('üé§üõë User interrupted AI! Detected:', partialText);
            hasInterruptedCurrentSpeech = true;
            
            // Stop AI speech immediately (voice interrupt = true)
            interruptAISpeech(true);
            
            // Show that AI is now listening
            showToast('üëÇ Listening...', 2000);
            
            // Reset interrupt flag after a short delay
            setTimeout(() => {
              hasInterruptedCurrentSpeech = false;
            }, 3000);
          }
        };

        recognizer.recognized = (s, e) => {
          setUserSpeaking(false);
          
          if (e.result.reason === sdk.ResultReason.RecognizedSpeech) {
            const text = e.result.text.trim();
            
            // Ignore empty or very short speech (noise/silence)
            if (!text || text.length < 2) {
              console.log('üé§ Ignoring empty/short speech');
              return;
            }
            
            // ‚ö†Ô∏è Check if this is just an interruption phrase
            if (isInterruptionPhrase(text)) {
              console.log('üé§üõë Detected interruption phrase (not an answer):', text);
              wasInterrupted = true;
              
              // If AI was speaking, it's already been interrupted
              // Just acknowledge and wait for the actual answer
              if (!isSpeaking && !aiTile.classList.contains('speaking')) {
                // AI already stopped - let user know we're ready
                showToast('üëÇ Go ahead, I\'m listening...', 2500);
                avatarStatus.textContent = 'Listening...';
                aiTile.classList.add('listening');
                setTimeout(() => aiTile.classList.remove('listening'), 4000);
              }
              return; // Don't send interruption phrase to LLM
            }
            
            // ‚ö†Ô∏è CRITICAL: Don't process speech while AI is speaking!
            // This prevents user interruptions from being treated as answers
            if (aiTile.classList.contains('speaking') || isSpeaking) {
              console.log('üé§‚è∏Ô∏è AI is speaking - checking if this is an answer or interruption');
              
              // Check if this looks like an actual answer (longer, substantive)
              const wordCount = text.split(/\s+/).length;
              if (wordCount > 6) {
                // This might be an actual answer - save it for after AI stops
                console.log('üé§üíæ Saving potential answer for later:', text);
                pendingUserSpeech = text;
                showToast('üí≠ Got it - will process your answer shortly...', 2000);
              } else {
                console.log('üé§‚è∏Ô∏è Short speech during AI - treating as interruption');
              }
              return; // Don't send to LLM while AI is talking
            }
            
            // Check if we have pending speech from an interruption
            if (pendingUserSpeech && wasInterrupted) {
              console.log('üé§üì§ Processing saved answer from interruption:', pendingUserSpeech);
              // Use the saved speech if it's longer/more substantive
              if (pendingUserSpeech.length > text.length) {
                text = pendingUserSpeech;
              }
              pendingUserSpeech = null;
              wasInterrupted = false;
            }
            
            const answerReceivedTime = Date.now();
            transcriptDiv.innerText = text;
            console.log('üé§ Recognized speech:', text);
            
            // Get the current question before fetching the next one
            const currentQuestion = questionDiv.innerText;

            setTimeout(async () => {
              // Double-check AI is not speaking before sending
              if (aiTile.classList.contains('speaking') || isSpeaking) {
                console.log('üé§‚è∏Ô∏è AI started speaking - aborting LLM call');
                return;
              }
              
              try {
                console.log('üì§ Sending to LLM...');
                showToast('ü§î AI is thinking...');
                
                // Save the Q&A exchange to transcript
                if (interviewSessionId && currentQuestion && currentQuestion !== 'Waiting to start...' && currentQuestion !== 'Click the microphone to begin') {
                  const responseTime = (answerReceivedTime - (window.lastQuestionTime || answerReceivedTime)) / 1000;
                  try {
                    await fetch(`${API_BASE}/api/interview/transcript`, {
                      method: 'POST',
                      headers: { 'Content-Type': 'application/json' },
                      body: JSON.stringify({
                        sessionId: interviewSessionId,
                        question: currentQuestion,
                        answer: text,
                        responseTimeSeconds: responseTime
                      })
                    });
                    console.log('üìù Transcript saved');
                  } catch (err) {
                    console.warn('Failed to save transcript:', err);
                  }
                }
                
                const resp = await fetch(`${API_BASE}/api/generate-question`, {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({ 
                    text, 
                    sessionId: interviewSessionId 
                  })
                });
                
                if (!resp.ok) {
                  throw new Error(`Backend error: ${resp.status} ${resp.statusText}`);
                }
                
                const data = await resp.json();
                console.log('üì• LLM Response data:', data);
                
                // Update question count from response
                if (data.questionCount !== null && data.questionCount !== undefined) {
                  questionCount = data.questionCount;
                  questionCounterDisplay.textContent = `Q: ${questionCount}/${maxQuestions}`;
                  console.log(`üìä Question ${questionCount}/${maxQuestions}`);
                  
                  // Change color as we approach the limit
                  if (questionCount >= maxQuestions - 3) {
                    questionCounterDisplay.style.background = 'rgba(234, 179, 8, 0.3)'; // Yellow warning
                  }
                  if (questionCount >= maxQuestions) {
                    questionCounterDisplay.style.background = 'rgba(239, 68, 68, 0.3)'; // Red
                  }
                }
                
                // Check if interview has ended
                if (data.isInterviewEnded) {
                  console.log('üèÅ Interview ended');
                  questionCounterDisplay.textContent = '‚úÖ Complete';
                  questionCounterDisplay.style.background = 'rgba(74, 222, 128, 0.3)';
                  showToast('Interview completed! Thank you for your time.', 10000);
                  
                  // Stop video recording and upload
                  stopVideoRecording();
                  
                  // Mark interview as complete on backend
                  if (interviewSessionId) {
                    fetch(`${API_BASE}/api/admin/interviews/${interviewSessionId}/complete`, {
                      method: 'POST'
                    }).catch(e => console.log('Failed to mark complete:', e));
                  }
                  
                  // Stop the microphone after the thank you message
                  setTimeout(() => {
                    if (recognizer) {
                      recognizer.stopContinuousRecognitionAsync();
                    }
                    stopBtn.click();
                  }, 2000);
                }
                
                const question = data.question;
                if (!question || question.trim() === '') {
                  console.warn('‚ö†Ô∏è Empty question received from LLM');
                  questionDiv.innerText = '(No question generated)';
                  return;
                }
                
                questionDiv.innerText = question;
                window.lastQuestionTime = Date.now(); // Track when question was asked
                console.log('üé§ LLM Response received, will speak:', question);
                
                // Use avatar if available, otherwise fall back to TTS
                if (avatarSynthesizer && isAvatarVideoDisplaying) {
                  console.log('üé≠ Speaking with Avatar...');
                  await speakWithAvatar(question);
                  console.log('‚úÖ speakWithAvatar completed');
                } else {
                  console.log('üîä Calling speakWithTTS...');
                  await speakWithTTS(question);
                  console.log('‚úÖ speakWithTTS completed');
                }
                
              } catch (error) {
                console.error('‚ùå Error processing response:', error);
                questionDiv.innerText = 'Error: ' + error.message;
                showToast('‚ùå Error: ' + error.message);
              }
            }, 0);
          } else {
            console.log("No speech recognized. Reason:", e.result.reason);
          }
        };

        recognizer.canceled = (s, e) => {
          console.error('canceled', e);
          statusSpan.textContent = 'Stopped';
          statusSpan.className = '';
          recordingIndicator.classList.remove('active');
          setUserSpeaking(false);
        };
        
        recognizer.startContinuousRecognitionAsync(
          () => {
            statusSpan.textContent = 'Recording';
            statusSpan.className = 'recording';
            startBtn.disabled = true;
            stopBtn.disabled = false;
            startBtn.classList.remove('primary');
            startBtn.classList.add('success');
            stopBtn.classList.remove('muted');
            stopBtn.classList.add('danger');
            recordingIndicator.classList.add('active');
            document.getElementById('user-status').textContent = 'Recording';
            showToast('üé§ Recording started - speak now!');
          },
          (err) => {
            console.error('Failed to start:', err);
            statusSpan.textContent = 'Error: ' + err;
            statusSpan.className = '';
            showToast('‚ùå Microphone error: ' + err);
          }
        );
      };

      stopBtn.onclick = () => {
        if (recognizer) {
          recognizer.stopContinuousRecognitionAsync(
            () => {
              statusSpan.textContent = 'Stopped';
              statusSpan.className = '';
              startBtn.disabled = false;
              stopBtn.disabled = true;
              startBtn.classList.add('primary');
              startBtn.classList.remove('success');
              stopBtn.classList.add('muted');
              stopBtn.classList.remove('danger');
              recordingIndicator.classList.remove('active');
              document.getElementById('user-status').textContent = 'Ready';
              setUserSpeaking(false);
              showToast('üõë Recording stopped');
            },
            (err) => {
              console.error('Failed to stop:', err);
            }
          );
        }
      };
    </script>
  </body>
</html>
